# -*- coding: utf-8 -*-
# file: redis_to_iceberg.py
# desc: Redis Stream â†’ Iceberg ì ì¬ íŒŒì´í”„ë¼ì¸ (ëª¨ë“  êµ¬ì„± í¬í•¨)
# author: ì†¡ë¯¼ì˜
# created: 2025-07-25

import os
import boto3
import time
import logging
import threading
import glob
import importlib
import redis
import pyarrow as pa
from schemas.click_event import click_arrow_fields
from schemas.keydown_event import keydown_arrow_fields

from config.iceberg import catalog, NAMESPACE_NAME


# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


# MinIO ì—°ê²° í™•ì¸
def check_minio_connection():
    """
    MinIO ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ë²„í‚· ëª©ë¡ì„ ë¡œê·¸ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

    Returns:
        None
    """
    try:
        s3 = boto3.client(
            's3',
            endpoint_url=os.getenv("CATALOG_S3_ENDPOINT", "http://minio:9000"),
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        )
        buckets = s3.list_buckets()
        logger.info(f"âœ… MinIO ì—°ê²° ì„±ê³µ: {[b['Name'] for b in buckets.get('Buckets', [])]}")
    except Exception as e:
        logger.error(f"ğŸš¨ MinIO ì—°ê²° ì‹¤íŒ¨: {e}")

check_minio_connection()

# Redis ì—°ê²°
try:
    r = redis.Redis(host='sv_redis', port=6379, decode_responses=True)
    r.ping()
    logger.info("âœ… Redis ì—°ê²° ì„±ê³µ")
except Exception as e:
    logger.error(f"ğŸš¨ Redis ì—°ê²° ì‹¤íŒ¨: {e}")

# ê³µí†µ ìƒìˆ˜
GROUP_NAME = "worker-group"
BATCH_SIZE = 100
# ---------------------------- SCHEMA AUTO-LOADER ----------------------------

SCHEMAS = {}
schema_funcs = {
    "click": click_arrow_fields,
    "keydown": keydown_arrow_fields
}

for event_type, func in schema_funcs.items():
    SCHEMAS[event_type] = func()

# ---------------------------- RECORD & STREAM ì²˜ë¦¬ ----------------------------

def convert_to_record(fields, schema_fields):
    """
    Redisì—ì„œ ì½ì€ í•„ë“œë¥¼ PyArrow ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ë ˆì½”ë“œ(dict)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        fields (dict): Redisì—ì„œ ìˆ˜ì‹ í•œ ì´ë²¤íŠ¸ í•„ë“œ
        schema_fields (list): PyArrow ìŠ¤í‚¤ë§ˆ í•„ë“œ ì •ì˜

    Returns:
        dict: ë³€í™˜ëœ ë ˆì½”ë“œ
    """
    record = {}
    for k, typ in schema_fields:
        v = fields.get(k)
        if typ == pa.bool_():
            record[k] = True if v == "True" else False
        elif typ in [pa.int32(), pa.int64(), pa.timestamp("ms")]:
            try:
                record[k] = int(v) if v not in [None, ""] else 0
            except Exception:
                record[k] = 0
        else:
            record[k] = v if v is not None else ""
    return record

def create_record_batch(batch, schema_fields):
    """
    ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ PyArrow RecordBatchë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        batch (list): dict í˜•íƒœì˜ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸
        schema_fields (list): PyArrow ìŠ¤í‚¤ë§ˆ í•„ë“œ ì •ì˜

    Returns:
        pa.RecordBatch: ìƒì„±ëœ RecordBatch
    """
    columns, names = [], []
    for name, typ in schema_fields:
        col = []
        for r_ in batch:
            val = r_.get(name)
            if typ in [pa.int32(), pa.int64(), pa.timestamp("ms")]:
                val = 0 if val in [None, ""] else int(val)
            elif typ == pa.bool_():
                val = bool(val)
            elif typ == pa.string():
                val = val or ""
            col.append(val)
        columns.append(pa.array(col, type=typ))
        names.append(name)
    return pa.RecordBatch.from_arrays(columns, schema=pa.schema([
        pa.field(name, typ, nullable=False) for name, typ in zip(names, [typ for _, typ in schema_fields])
    ]))

def ensure_consumer_group(stream_name):
    """
    ì§€ì •ëœ Redis Streamì— ëŒ€í•´ Consumer Groupì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        stream_name (str): Redis Stream ì´ë¦„

    Returns:
        None
    """
    try:
        r.xgroup_create(stream_name, GROUP_NAME, id='0', mkstream=True)
        logger.info(f"âœ… ì–´í”Œ êµ¬ë„ ìƒì„±: {stream_name}:{GROUP_NAME}")
    except Exception as e:
        if "BUSYGROUP" in str(e):
            logger.info(f"âœ… ì–´í”Œ êµ¬ë„ ê¸°ì¡´: {stream_name}:{GROUP_NAME}")
        else:
            logger.error(f"ğŸš¨ ì–´í”Œ êµ¬ë„ ìƒì„± ì‹¤íŒ¨: {stream_name}: {e}")


def delete_from_stream(stream_name, ids):
    """
    Iceberg ì ì¬ í›„ Redis Streamì—ì„œ ì²˜ë¦¬ëœ ë©”ì‹œì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.

    Args:
        stream_name (str): Redis Stream ì´ë¦„
        ids (list): ì‚­ì œí•  ë©”ì‹œì§€ ID ë¦¬ìŠ¤íŠ¸

    Returns:
        None
    """
    time.sleep(5)  # ì ì¬ ì•ˆì •ì„±ì„ ìœ„í•´ ëŒ€ê¸°
    for msg_id in ids:
        r.xdel(stream_name, msg_id)
    logger.info(f"ğŸ—‘ï¸ [{stream_name}] Streamì—ì„œ {len(ids)}ê±´ ì‚­ì œ")


def process_stream(stream_name):
    """
    ì§€ì •ëœ Redis Streamì„ ì§€ì†ì ìœ¼ë¡œ ì†Œë¹„í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ Icebergì— ì ì¬í•©ë‹ˆë‹¤.

    Args:
        stream_name (str): Redis Stream ì´ë¦„

    Returns:
        None
    """
    logger.info(f"ğŸš€ ìŠ¤íŠ¸ë¦¼ ì†Œë¹„ ì‹œì‘: {stream_name}")
    ensure_consumer_group(stream_name)

    batch, ids = [], []
    last_flush = time.time()
    event_type = stream_name.replace("_events", "")
    schema_fields = SCHEMAS[event_type]
    table = catalog.load_table(f"{NAMESPACE_NAME}.{event_type}_events")

    while True:
        try:
            resp = r.xreadgroup(GROUP_NAME, "consumer-1", {stream_name: ">"}, count=BATCH_SIZE, block=5000)
            now = time.time()

            if resp:
                for _, messages in resp:
                    for msg_id, fields in messages:
                        record = convert_to_record(fields, schema_fields)
                        batch.append(record)
                        ids.append(msg_id)

            if len(batch) >= BATCH_SIZE or (batch and now - last_flush >= 5):
                if batch:
                    rb = create_record_batch(batch, schema_fields)
                    table.append(pa.Table.from_batches([rb]))
                    logger.info(f"ğŸ“‹ [{stream_name}] ë°°ì¹˜ ì ì¬ ì™„ë£Œ: {len(batch)}ê°œ")

                    # ë©”ì‹œì§€ ì‚­ì œë¥¼ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬
                    threading.Thread(target=delete_from_stream, args=(stream_name, ids.copy())).start()
                    batch.clear()
                    ids.clear()
                    last_flush = now

        except Exception as e:
            logger.error(f"ğŸš¨ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {stream_name}: {e}")
            time.sleep(2)

# ---------------------------- MAIN ----------------------------

if __name__ == "__main__":
    streams = [f"{k}_events" for k in SCHEMAS.keys()]
    threads = []
    for stream in streams:
        t = threading.Thread(target=process_stream, args=(stream,))
        t.start()
        threads.append(t)
    for t in threads:
        t.join()