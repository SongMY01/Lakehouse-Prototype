# -*- coding: utf-8 -*-
# file: services/event_loader.py
# desc: Redis Streamì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì½ì–´ Icebergì— ì ì¬í•˜ëŠ” ë°°ì¹˜ ë¡œë”
# author: minyoung.song
# created: 2025-07-23

import os
import glob
import threading
import time
import logging
import importlib
import redis
import pyarrow as pa

from config.redis import r
from config.iceberg import catalog, NAMESPACE_NAME

# ğŸ”· ë¡œê¹… ì„¤ì •
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=log_level,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)

# ğŸ”· ì»¨ìŠˆë¨¸ ê·¸ë£¹ ë° ë°°ì¹˜ ì„¤ì •
GROUP_NAME = "worker-group"
CONSUMER_NAME = "worker-1"
BATCH_SIZE = 10
TIMEOUT_SEC = 5

# ğŸ”· schemas í´ë”ì˜ ìŠ¤í‚¤ë§ˆ ëª¨ë“ˆì„ ìë™ ë¡œë“œí•´ SCHEMAS êµ¬ì„±
SCHEMAS = {}
schemas_path = os.path.join(os.path.dirname(__file__), "..", "schemas")
schema_files = glob.glob(os.path.join(schemas_path, "*_event.py"))

for f in schema_files:
    basename = os.path.basename(f).replace(".py", "")
    event_type = basename.replace("_event", "")
    module_name = f"schemas.{basename}"
    mod = importlib.import_module(module_name)
    arrow_func = getattr(mod, f"{event_type}_arrow_fields")
    SCHEMAS[event_type] = arrow_func()

def convert_to_record(fields, schema_fields):
    """
    Redis ë©”ì‹œì§€ë¥¼ Iceberg ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ë ˆì½”ë“œ(dict)ë¡œ ë³€í™˜

    Args:
        fields (dict): Redisì—ì„œ ì½ì€ ë°ì´í„°
        schema_fields (List[Tuple[str, pa.DataType]]): Iceberg í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ

    Returns:
        dict: Icebergì— ì ì¬ ê°€ëŠ¥í•œ ë ˆì½”ë“œ
    """
    record = {}
    for k, typ in schema_fields:
        v = fields.get(k)
        # íƒ€ì…ì— ë§ê²Œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        if typ == pa.bool_():
            record[k] = True if v == "True" else False
        elif typ == pa.int32():
            try:
                record[k] = int(v) if v not in [None, ""] else 0
            except (ValueError, TypeError):
                record[k] = 0
        elif typ == pa.timestamp("ms"):
            try:
                record[k] = int(v) if v not in [None, ""] else 0
            except (ValueError, TypeError):
                record[k] = 0
        else:
            record[k] = v if v is not None else ""
    return record

def create_record_batch(batch, schema_fields):
    """
    batch ë°ì´í„°ë¥¼ PyArrow RecordBatchë¡œ ìƒì„±

    Args:
        batch (List[dict]): ë³€í™˜ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸
        schema_fields (List[Tuple[str, pa.DataType]]): Iceberg í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ

    Returns:
        pyarrow.RecordBatch: Icebergì— ì ì¬í•  RecordBatch
    """
    columns, names = [], []
    for name, typ in schema_fields:
        col = []
        for r_ in batch:
            val = r_.get(name)
            if typ == pa.int32():
                if val in [None, ""]:
                    val = 0
                elif not isinstance(val, int):
                    try:
                        val = int(val)
                    except (ValueError, TypeError):
                        val = 0
            if typ == pa.bool_():
                val = bool(val) if val not in [None, ""] else False
            if typ == pa.string() and val is None:
                val = ""
            if typ == pa.timestamp("ms"):
                if val in [None, ""]:
                    val = 0
                elif not isinstance(val, int):
                    try:
                        val = int(val)
                    except (ValueError, TypeError):
                        val = 0
            col.append(val)
        columns.append(pa.array(col, type=typ))
        names.append(name)
    return pa.record_batch(columns, names=names)

def delete_from_stream(stream_name, ids):
    """
    Iceberg ì ì¬ ì™„ë£Œ í›„ Redisì—ì„œ ë©”ì‹œì§€ ì‚­ì œ
    """
    time.sleep(5)  # ì ì¬ ì•ˆì •ì„±ì„ ìœ„í•´ ëŒ€ê¸°
    for msg_id in ids:
        r.xdel(stream_name, msg_id)
    logger.info(f"ğŸ—‘ï¸ [{stream_name}] Streamì—ì„œ {len(ids)}ê±´ ì‚­ì œ")


# --- ë¦¬íŒ©í† ë§: ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„±, ë©”ì‹œì§€ ì²˜ë¦¬, ë°°ì¹˜ ì ì¬ í•¨ìˆ˜í™” ---

def ensure_consumer_group(stream_name):
    """
    ì§€ì •í•œ Redis Streamì— ì»¨ìŠˆë¨¸ ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒì„±
    """
    try:
        r.xgroup_create(stream_name, GROUP_NAME, id='0', mkstream=True)
        logger.info(f"âœ… ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„±: {stream_name}:{GROUP_NAME}")
    except redis.exceptions.ResponseError as e:
        if "BUSYGROUP" in str(e):
            logger.info(f"âœ… ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì´ë¯¸ ì¡´ì¬: {stream_name}:{GROUP_NAME}")
        else:
            raise e

def process_messages(msgs, stream_name, batch, ids):
    """
    Redis ë©”ì‹œì§€ë¥¼ ì½ì–´ ë°°ì¹˜ì— ì¶”ê°€í•˜ê³  ack ì²˜ë¦¬
    """
    table_name = None
    schema_fields = None
    for _, messages in msgs:
        for msg_id, fields in messages:
            event_type = stream_name.replace("_events", "")
            table_name = f"{NAMESPACE_NAME}.{event_type}_events"
            schema_fields = SCHEMAS[event_type]

            record = convert_to_record(fields, schema_fields)
            batch.append(record)
            ids.append(msg_id)
            r.xack(stream_name, GROUP_NAME, msg_id)
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— í•„ìš”í•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜
    if batch:
        return table_name, schema_fields
    return None, None

def write_batch_to_iceberg(batch, schema_fields, table_name, stream_name):
    """
    ë°°ì¹˜ë¥¼ Icebergì— ì ì¬í•˜ê³  Redis ë©”ì‹œì§€ë¥¼ ì‚­ì œ
    """
    logger.info(f"ğŸ“‹ [{stream_name}] ë°°ì¹˜ ì ì¬ ì‹œì‘: {len(batch)}ê±´")
    record_batch = create_record_batch(batch, schema_fields)
    try:
        table = catalog.load_table(table_name)
        table.append(pa.Table.from_batches([record_batch]))
        logger.info(f"âœ… [{stream_name}] Iceberg ì ì¬ ì™„ë£Œ: {len(batch)}ê±´")
    except Exception as e:
        logger.error(f"ğŸš¨ Iceberg í…Œì´ë¸” ë¡œë“œ ì‹¤íŒ¨: {table_name}\n{e}")

def process_stream(stream_name):
    """
    ì§€ì •ëœ Redis Streamì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½ì–´ Icebergì— ì ì¬
    """
    ensure_consumer_group(stream_name)

    batch, ids = [], []
    last_flush = time.time()
    table_name, schema_fields = None, None

    while True:
        msgs = r.xreadgroup(
            groupname=GROUP_NAME,
            consumername=CONSUMER_NAME,
            streams={stream_name: '>'},
            count=BATCH_SIZE,
            block=2000
        )
        now = time.time()
        if msgs:
            table_name, schema_fields = process_messages(msgs, stream_name, batch, ids)

        if len(batch) >= BATCH_SIZE or (batch and now - last_flush >= TIMEOUT_SEC):
            if table_name and schema_fields:
                write_batch_to_iceberg(batch, schema_fields, table_name, stream_name)
            else:
                logger.warning(f"ğŸš¨ [{stream_name}] ë°°ì¹˜ ì ì¬ ì •ë³´ ëˆ„ë½ (table_name or schema_fields ì—†ìŒ)")

            threading.Thread(
                target=delete_from_stream,
                args=(stream_name, ids.copy(),)
            ).start()
            batch.clear()
            ids.clear()
            last_flush = now

if __name__ == "__main__":
    # schemas í´ë”ì˜ *_event.py íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ Stream ì´ë¦„ì„ ìƒì„±í•´ ê° Streamì„ ë…ë¦½ ì“°ë ˆë“œì—ì„œ ì‹¤í–‰
    streams = [os.path.basename(f).replace("_event.py", "_events") for f in schema_files]
    threads = []
    for stream in streams:
        t = threading.Thread(target=process_stream, args=(stream,))
        t.start()
        threads.append(t)
    for t in threads:
        t.join()