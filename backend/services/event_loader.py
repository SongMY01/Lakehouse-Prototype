# -*- coding: utf-8 -*-
# file: services/event_loader.py
# desc: Redis Streamì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì½ì–´ Icebergì— ì ì¬í•˜ëŠ” ë°°ì¹˜ ë¡œë”
# author: minyoung.song
# created: 2025-07-23

import os
import glob
import threading
import time
import logging
import importlib
import redis
import pyarrow as pa

from config.redis import r
from config.iceberg import catalog, NAMESPACE_NAME

# ğŸ”· ë¡œê¹… ì„¤ì •
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=log_level,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)

# ğŸ”· ì»¨ìŠˆë¨¸ ê·¸ë£¹ ë° ë°°ì¹˜ ì„¤ì •
GROUP_NAME = "worker-group"
CONSUMER_NAME = "worker-1"
BATCH_SIZE = 10
TIMEOUT_SEC = 5

# ğŸ”· ì´ë²¤íŠ¸ë³„ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ìë™í™”)
SCHEMAS = {}

schemas_path = os.path.join(os.path.dirname(__file__), "..", "schemas")
schema_files = glob.glob(os.path.join(schemas_path, "*_event.py"))

for f in schema_files:
    basename = os.path.basename(f).replace(".py", "")
    event_type = basename.replace("_event", "")  # ex: click, keydown
    module_name = f"schemas.{basename}"
    mod = importlib.import_module(module_name)
    arrow_func = getattr(mod, f"{event_type}_arrow_fields")
    SCHEMAS[event_type] = arrow_func()

def convert_to_record(fields, schema_fields):
    """
    Redis ë©”ì‹œì§€ë¥¼ Iceberg ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ë ˆì½”ë“œ(dict)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        fields (dict): Redisì—ì„œ ì½ì€ ì›ë³¸ í•„ë“œ ë°ì´í„°
        schema_fields (List[Tuple[str, pa.DataType]]): Iceberg í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆ

    Returns:
        dict: Icebergì— ì ì¬í•  ë ˆì½”ë“œ
    """
    record = {}
    for k, typ in schema_fields:
        v = fields.get(k)
        # ì•ˆì „í•œ íƒ€ì… ë³€í™˜
        if typ == pa.bool_():
            record[k] = True if v == "True" else False
        elif typ == pa.int32():
            try:
                record[k] = int(v) if v not in [None, ""] else 0
            except (ValueError, TypeError):
                record[k] = 0
        elif typ == pa.timestamp("ms"):
            try:
                record[k] = int(v) if v not in [None, ""] else 0
            except (ValueError, TypeError):
                record[k] = 0
        else:
            record[k] = v if v is not None else ""
    return record

def create_record_batch(batch, schema_fields):
    """
    batch ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PyArrow RecordBatchë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        batch (List[dict]): ë³€í™˜ëœ ë ˆì½”ë“œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        schema_fields (List[Tuple[str, pa.DataType]]): Iceberg í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆ

    Returns:
        pyarrow.RecordBatch: Icebergì— ì ì¬í•  RecordBatch ê°ì²´
    """
    columns, names = [], []
    for name, typ in schema_fields:
        col = []
        for r_ in batch:
            val = r_.get(name)
            if typ == pa.int32():
                if val in [None, ""]:
                    val = 0
                elif not isinstance(val, int):
                    try:
                        val = int(val)
                    except (ValueError, TypeError):
                        val = 0
            if typ == pa.bool_():
                val = bool(val) if val not in [None, ""] else False
            if typ == pa.string() and val is None:
                val = ""
            if typ == pa.timestamp("ms"):
                if val in [None, ""]:
                    val = 0
                elif not isinstance(val, int):
                    try:
                        val = int(val)
                    except (ValueError, TypeError):
                        val = 0
            col.append(val)
        columns.append(pa.array(col, type=typ))
        names.append(name)
    return pa.record_batch(columns, names=names)

def delete_from_stream(stream_name, ids):
    """
    Redis Streamì—ì„œ ì²˜ë¦¬ëœ ë©”ì‹œì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    time.sleep(5)  # Iceberg ì ì¬ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦° í›„ Redisì—ì„œ ë©”ì‹œì§€ ì‚­ì œ
    for msg_id in ids:
        r.xdel(stream_name, msg_id)
    logger.info(f"ğŸ—‘ï¸ [{stream_name}] Streamì—ì„œ {len(ids)}ê±´ ì‚­ì œ")

def process_stream(stream_name):
    """
    ì§€ì •ëœ Redis Streamì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì½ì–´ Icebergì— ì ì¬í•©ë‹ˆë‹¤.
    """
    # ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ íŒ¨ìŠ¤)
    try:
        r.xgroup_create(stream_name, GROUP_NAME, id='0', mkstream=True)
        logger.info(f"âœ… ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„±: {stream_name}:{GROUP_NAME}")
    except redis.exceptions.ResponseError as e: # type: ignore
        if "BUSYGROUP" in str(e):
            logger.info(f"âœ… ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì´ë¯¸ ì¡´ì¬: {stream_name}:{GROUP_NAME}")
        else:
            raise e

    batch = []
    ids = []
    last_flush = time.time()

    while True:
        # Redisì—ì„œ ì´ë²¤íŠ¸ ì½ê¸°
        msgs = r.xreadgroup(
            groupname=GROUP_NAME,
            consumername=CONSUMER_NAME,
            streams={stream_name: '>'},
            count=BATCH_SIZE,
            block=2000
        )
        now = time.time()
        if msgs:
            for _, messages in msgs: # type: ignore
                for msg_id, fields in messages:
                    event_type = stream_name.replace("_events", "")
                    table_name = f"{NAMESPACE_NAME}.{event_type}_events"
                    schema_fields = SCHEMAS[event_type]

                    # Redis ë©”ì‹œì§€ë¥¼ Icebergìš© ë ˆì½”ë“œë¡œ ë³€í™˜
                    record = convert_to_record(fields, schema_fields)

                    batch.append(record)
                    ids.append(msg_id)
                    r.xack(stream_name, GROUP_NAME, msg_id)

        # ë°°ì¹˜ í¬ê¸° ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ Icebergì— ì ì¬
        if len(batch) >= BATCH_SIZE or (batch and now - last_flush >= TIMEOUT_SEC):
            logger.info(f"ğŸ“‹ [{stream_name}] ë°°ì¹˜ ì ì¬ ì‹œì‘: {len(batch)}ê±´")
            record_batch = create_record_batch(batch, schema_fields)
            try:
                table = catalog.load_table(table_name)
                table.append(pa.Table.from_batches([record_batch]))
                logger.info(f"âœ… [{stream_name}] Iceberg ì ì¬ ì™„ë£Œ: {len(batch)}ê±´")
            except Exception as e:
                logger.error(f"ğŸš¨ Iceberg í…Œì´ë¸” ë¡œë“œ ì‹¤íŒ¨: {table_name}\n{e}")

            # ì ì¬ í›„ Redisì—ì„œ ë©”ì‹œì§€ ì‚­ì œ (ë°±ê·¸ë¼ìš´ë“œ)
            threading.Thread(
                target=delete_from_stream,
                args=(stream_name, ids.copy(),)
            ).start()
            batch.clear()
            ids.clear()
            last_flush = now

if __name__ == "__main__":
    # schemas í´ë”ì˜ *_event.py íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ Stream ì´ë¦„ì„ ìë™ ìƒì„±í•˜ì—¬ ê° Streamì„ ë…ë¦½ì ì¸ ì“°ë ˆë“œì—ì„œ ì‹¤í–‰  
    streams = [os.path.basename(f).replace("_event.py", "_events") for f in schema_files]
    threads = []
    for stream in streams:
        t = threading.Thread(target=process_stream, args=(stream,))
        t.start()
        threads.append(t)
    for t in threads:
        t.join()